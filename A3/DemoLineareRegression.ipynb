{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Einführung Machine Learning\n",
    "### Sommersemester 2024\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressionsanalyse\n",
    "\n",
    "Die Regressionsanalyse ist ein statistisches Verfahren, mit dem man eine beobachtete, abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären versucht.\n",
    "Die **Lineare Regression** ist dabei ein Sonderfall der Regressionsanalyse, bei dem zur Beschreibung der Daten ein lineares Modell herangezogen wird. Das bedeutet, die abhängige Variable wird als Linearkombination der (mit den Regressionskoeffizienten) gewichteten unabhängigen Variablen beschrieben.\n",
    "Im Gegensatz zur Klassifikation, werden bei der Regressionsanalyse immer Werte für einer kontinuierlichen Variable vorhergesagt.\n",
    "Beispiele sind etwa Schätzer für folgende Fragestellungen:\n",
    "- Was ist der angemessene Preis für ein Produkt\n",
    "- Wie groß ist der Absatzmarkt für ein neues Produkt\n",
    "- Wie lang ist die Halbarkeit eines Gerätes\n",
    "- Wie hoch sind die Kosten eines bestimmten Projektes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Daten aufbereiten und analysieren\n",
    "\n",
    "Am Anfang aller Machine Learning Projekte stehen die Daten.\n",
    "Ohne eine geeignete Datenbasis können Algorithmen keine neuen Einsichten oder Voraussagen treffen.\n",
    "\n",
    "In diesem Modul werden wir hauptsächlich **strukturierte Daten** betrachten.\n",
    "Strukturierte Daten orientieren sich nach einem vorgegebenen Format.\n",
    "Eine Tabelle etwa ist ein strukturierter Datensatz, dann alle Werte sind einer bestimmten Kombination aus Zeile und Spalte im Datensatz zugeordnet.\n",
    "Die einzelnen Spalten beschreiben dabei häufig *Kategorien* bzw. *Eigenschaften* der Daten.\n",
    "Die Zeilen listen dann einzelnen Datenpunkte mit Werten für die entsprechenden Kategorien in den Spalten.\n",
    "\n",
    "Unter **unstrukturierten Daten** versteht man Datensätze, die keine festgelegte Struktur besitzen.\n",
    "Ein unstrukturierter Datensatz kann zwar eine Struktur in sich tragen, sie ist aber in dem vorliegenden Format nicht abgebildet und damit nicht direkt erkennbar.\n",
    "Es gibt Machine Learning Algorithmen, die Strukturen in unstrukturierten Datensätzen aufspüren und sichtbar machen.\n",
    "Zu den unstrukturierten Daten zählt man üblicherweise geschriebene und gesprochene Sprache (Texte und Sprache), Bilder und Videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Arbeitsblatt werden wir Daten betrachten, die von dem bekannten Online-Marktplatz für Gebrauchtfahrzeuge [AutoScout24](https://www.autoscout24.de) stammen.\n",
    "Die Daten wurden am 11.05.2015 für ein Projekt an der ETH Zürich gesammelt.\n",
    "\n",
    "Der Datensatz liegt als CSV (comma-separated values) Datei vor, einem gängigen Format für tabellarische Daten.\n",
    "Wir benutzen die Python **Pandas** Bibliothek, um den Datensatz einzulesen und um ihn zu verarbeiten.\n",
    "Die Funktion `read_csv` importiert eine CSV-Datei in ein `DataFrame` Objekt.\n",
    "Da der Datensatz einige nicht-ASCII Zeichen enthält, verwenden wir das ISO-8859-1 (Latin-1) Zeichenformat beim Import.\n",
    "Mit `head(10)` geben wir die ersten 10 Zeilen des Datensatzes aus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://github.com/fh-swf-hgi/ml/raw/main/u2/vw_golf_2015_complete.csv\"\n",
    "dfile = \"./vw_golf_2015_complete.csv\"\n",
    "\n",
    "if not os.path.isfile(dfile):\n",
    "    urllib.request.urlretrieve(url, dfile)\n",
    "    \n",
    "df = pd.read_csv(\"vw_golf_2015_complete.csv\", encoding = \"ISO-8859-1\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einige weitere Informationen, z.B. die Namen und Datentypen der Spalten, erhalten wir mit der Funktion `info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `describe()`erhalten wir einige Statistik-Informationen über die Spalten des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datei enthält mit ca. 70.000 Datenpunkten recht viele Einträge.\n",
    "Für unser Beispiel wollen wir den Datensatz etwas reduzieren.\n",
    "Dazu wählen wir ein spezielles Automodell aus.\n",
    "In der folgenden Zelle werden die Spalten *marke* und *modell* ausgewählt und in der resultierenden Tabelle alle doppelten Einträge verworfen. Wir erhalten so einen Dataframe der alle Modelltypen einmalig enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typen = df[[\"marke\", \"modell\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typen.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Neue Tabelle enthält 705 unterschiedliche Typen.\n",
    "Da die Indizes aus der ursprünglichen Tabelle übernommen werden, indizieren wir den neuen Datensatz neu durch.\n",
    "Von dem resultierenden Dataframe geben wir die ersten 10 Zeilen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typen.reset_index(drop=True, inplace=True)\n",
    "df_typen.loc[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `query(\"modell=='Golf VI'\")` wählen wir die Golf-VI Modelle aus der Liste aus und speichern die Datenpunkte in einen neuen `DataFrame`.\n",
    "\n",
    "Auch die Anzahl der Merkmale (Spalten) ist mit 54 recht hoch.\n",
    "Daher reduzieren wir die Spalten des DataFrames auf die 10 Merkmale im Array `merkmale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df.query(\"modell=='Golf VI'\")\n",
    "\n",
    "merkmale = ['klimaanlage','nichtraucherfahrzeug','anhaengerkupplung',\n",
    "            'preis','erstzulassung_jahr','leistung','hubraum',\n",
    "            'sitzplaetze','tueren','kilometer'\n",
    "           ]\n",
    "\n",
    "daten = df_select[merkmale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daten.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Visualisieren und mit Statistischen Kennwerten beschreiben\n",
    "\n",
    "Als Nächstes möchten wir einen Eindruck erhalten, wie die einzelnen Merkmale miteinander zusammenhängen.\n",
    "Ist die Anzahl der Merkmale überschaubar, bietet es sich an, **Streudiagramme** (engl. *scatter plots*) für alle Paare von Merkmalen zu zeichnen.\n",
    "\n",
    "Streudiagramm werden die Elemente zweier Datenreihen paarweise (als X- und Y-Werte) in ein kartesisches Koordinatensystem eingetragen.\n",
    "Dadurch ergibt sich eine sogenannte Punktwolke, die anzeigt, in welcher Abhängigkeit die beiden Datenreihen zueinander stehen.\n",
    "\n",
    "Für Python gibt es verschiedene Bibliotheken, mit denen man Streudiagramme zeichnen kann.\n",
    "Da unsere Daten als Pandas `DataFrame` vorliegen, bietet sich in unserem Fall die Funktion `scatter_matrix` aus dem Paket `pandas.plotting` an.\n",
    "Da wir in ein kartesisches Koordinatensystem nur numerische Koordinaten eintragen können, reduzieren wir den DataFrame auf Spalten, die entweder als 64-bit `float` oder `int` Typen vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "scatter_matrix(daten.select_dtypes(include=['float64','int64']), figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Abbildung zeigt nun eine Vielzahl von *Punktwolken*.\n",
    "Uns interessiert hier vor allem die erste Zeile (oder die erste Spalte, da die Graphen an der Hauptdiagonalen gespiegelt sind).\n",
    "Dort sind die Punktwolken abgebildet, bei denen die eine Datenreihe der Preis des Autos ist.\n",
    "Wir wollen später eine **Schätzfunktion** trainieren, die möglichst gut den Preis eines Gebrauchtfahrzeugs vorhersagen kann.\n",
    "Daher müssen wir herausfinden, mit welchen anderen Merkmalen der Preis möglichst eng linear *korreliert*.\n",
    "Grafisch betrachtet, suchen wir nach Punktwolken, die sich möglichst eng anhand **einer** Geraden orientieren.\n",
    "\n",
    "Erfreulicherweise sind wir nicht auf die visuelle Analyse der Daten angewiesen.\n",
    "Der **Korrelationskoeffizient** $r$ ist eine Kenngröße, die angibt, wie stark die Werte zweier Datenreihen $x$ und $y$ miteinander linear korrelieren.\n",
    "Der Wert von $r$ liegt immer in der Grenzen $[-1,1]$.\n",
    "Dabei bedeutet ein sehr niedriger (etwa $r<-0.8$) oder ein sehr hoher Wert (etwa $r>0.8$), dass die Reihen korrelieren, es also einen statistischen Zusammenhang der Datenreihen gibt.\n",
    "Ein Wert von $r$ um den Nullpunkt (etwa $-0.5<r<0.5$) bedeutet, dass es keinen signifikanten Zusammenhang gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Berechnung der **Korrelationsmatrix** mit der Pandas Funktion `corr()`, nehmen wir den Datensatz, der alle Merkmale enthält.\n",
    "Aus der berechneten Matrix selektieren wir die Spalte *Preis* und geben die Korrelationskoeffizienten in absteigend sortierter Reihenfolge aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_select.corr()\n",
    "corr_matrix[\"preis\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da für uns die absolut größten Werte interessant sind, wenden wir die Betragsfunktion `abs()` auf die Werte an und geben die sortierte Liste erneut aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"preis\"].abs().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe: Berechnen Sie den Korrelationskoeffizienten $\\rho$ für die Spalten *preis* und *kilometer* \"per Hand\"\n",
    "Ziehen Sie dazu zuerst aus dem `DataFrame` `temp_df` mit dem Attribut `values` die Werte der (bereinigten) Datenreihen in NumPy Arrays.\n",
    "Sie können zur Berechnung die NumPy Funktionen `mean()` (für den Mittelwert, bzw. Erwartungswert $\\mu$) und `std()` (für die Standardabweichung $\\sigma$) benutzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\rho(X,Y) = \\Large \\frac{cov(X,Y)}{\\sigma X\\sigma Y}$  $\\,$   mit  $\\,$   $ \\large cov(X,Y)=E\\left[ (X-\\mu X) (Y-\\mu Y) \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "temp_df = df.query(\"modell=='Golf VI'\")[[\"preis\",\"kilometer\"]].dropna()\n",
    "ty = temp_df[\"preis\"].values\n",
    "tx = temp_df[\"kilometer\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cor_xy_tests",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cov_xy(x, y):\n",
    "    ''' Berechne die Kovarianz cov(x,y)\n",
    "        der beiden Merkmale x und y\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "cov_xy(tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e3d08f5ca1b18941",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "test_x0 = np.array([  1.,  -4.,   7.,  -6., -14.,  -1.,  -3., -33.,   6.,   3.])\n",
    "test_y0 = np.array([  2.,  -7., -14.,   3.,  -9.,  19.,  14.,  -3., -11., -23.])\n",
    "\n",
    "assert cov_xy(test_x0, test_y0) == -18.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55c5c76d61197e89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_r(x, y):\n",
    "    ''' Berechne den Korrelationskoeffizienten \n",
    "        roh(x, y) der beiden Merkmale x und y\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "calc_r(tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cafa64135563fb3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert np.around(calc_r(test_x0, test_y0), 4) == -0.1362"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy stellt die Funktion `corrcoef()` zur Verfügung.\n",
    "Damit können Sie ihr Ergebmis überprüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(tx,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten selektieren und aufbereiten\n",
    "Wir wollen uns für die weitere Analyse auf die Merkmale *erstzulassung_jahr*, *kilometer* und *leistung* konzentrieren, da sie die höchste Korrelation mit dem Merkmal *preis* aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_6_kompl = df.query(\"modell=='Golf VI'\")\n",
    "golf_6_num = golf_6_kompl[[\"preis\",\"erstzulassung_jahr\",\"kilometer\",\"leistung\"]]\n",
    "golf_6_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir den Datensatz auf diese Merkmale reduzieren, fällt auf, dass nicht alle Datenreihen die gleiche Anzahl an Werten beinhalten.\n",
    "Diese \"Lücken\" im Datensatz sind kritisch für die weiteren Schritte.\n",
    "Viele Algorithmen kommen mit unvollständigen Daten, bzw `NaN` Elementen nicht zurecht.\n",
    "Daher eliminieren wir die Datenpunkte, die ein `NaN` enthalten mit der Funktion `dropna()` aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_6_num = golf_6_num.dropna()\n",
    "golf_6_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir für die ausgewählten Merkmale Streudiagramme plotten, sehen wir, dass sich für den Preis Abhängigkeiten der Art **\"je mehr, desto mehr\"** (*erstzulassung_jahr*, *leistung*) sowie **\"je mehr, desto weniger\"** (*kilometer*) abzeichnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(golf_6_num, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wollen wir den Datensatz in 2 Teile aufteilen.\n",
    "Einen **Trainingsdatensatz** sowie einen **Testdatensatz**.\n",
    "Mit dem ersten *trainieren* wir unser Modell.\n",
    "Mit dem zweiten Datensatz testen wir die trainierte Modellfunktion.\n",
    "Mit diesem Ansatz adressieren wir das Problem des **Overfittings** (Überanpassung).\n",
    "Es kann beim Trainieren des Modells dazu kommen, dass der Lern-Algorithmus die Modell-Parameter zu sehr auf den Trainingsdatensatz anpasst.\n",
    "Durch vorhalten eines separaten Datensatzes, der nicht zum Trainieren benutzt wird, kann dieses Problem erkannt werden.\n",
    "\n",
    "Es ist üblich, etwa 1/5 bis 1/3 der Daten für den Trainingsdatensatz und die restlichen Daten für den Testdatensatz zu verwenden.\n",
    "Um die Daten aufzuteilen verwenden wir die Funktion `train_test_split` aus dem *sklearn* Modul `model_selection`.\n",
    "Mit dem Parameter `test_size` bestimmen wir den Anteil der Testdaten. Mit `random_state` wird der Zufallszahlengenerator initialisiert, über den die Datenpunkte ausgewählt werden. Legt man `random_state` fest, so wird immer die gleiche Aufteilung vorgenommen. Dies hat den Vorteil, dass die Analysen vergleichbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = golf_6_num['preis']\n",
    "#X = golf_6_num.drop(['preis'],1)\n",
    "# Wir wählen als einziges Merkmal die gefahrenen Kilometer aus\n",
    "X = golf_6_num['kilometer']\n",
    "\n",
    "# Da X nur ein Merkmal enthält, wird es von Pandas zur \"Series\" gemacht\n",
    "# Wir benötigen aber einen DataFrame\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "\n",
    "Bisher haben wir ausschließlich den Datensatz grafisch und statistisch analysiert, aber noch kein Machine Learning Algorithmus angewendet.\n",
    "Wir sind jetzt bereit, aus den gesammelten Daten eine *Schätzfunktion* abzuleiten.\n",
    "Diese Funktion soll auf Grundlage eines oder mehrerer Merkmale eine Schätzung für den Preis des Fahrzeugs ausgeben.\n",
    "Diese Schätzung soll so ausfallen, dass sie möglichst gut zu den bestehenden Datenpunkten passt.\n",
    "damit könnte man z.B. der *Richtpreis* für eine neue Annonce berechnen oder bestimmen, ob ein Angebot im Vergleich eher teuer oder günstig ist.\n",
    "\n",
    "Um diese Schätzfunktion zu entwickeln, stürzen wir uns auf die Zusammenhänge, die wir in der Analysephase herausgearbeitet haben und trainieren auf dieser Grundlage ein lineares Regressionsmodell.\n",
    "Zu Beginn betrachten wir den einfachsten Fall, die sogenannte **Univariate Lineare Regression**.\n",
    "Hierbei wird die abhängige Variable *preis* durch eine einzige unabhängige Variable, in diesem Fall den Kilometerstand, erklärt.\n",
    "Die Modellfunktion lautet entsprechend, $h_{\\Theta}(x)=\\Theta_0+\\Theta_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist durchaus möglich, das Regressionsmodell mit den Mitteln, die wir bisher kennengelernt haben (NumPy, Pandas), aufzustellen und zu trainieren.\n",
    "Um die Trainingsalgorithmen kennenzulernen, werden wir das zu einem späteren Zeitpunkt auch tun.\n",
    "Allerdings ist es in der Praxis eher unüblich, Machine Learning (ML) Modelle \"per Hand\" zu entwickeln.\n",
    "Es gibt, insbesondere für Python, eine Vielzahl von Bibliotheken, mit denen ML Modelle effizient trainiert werden können.\n",
    "\n",
    "Eine weit verbreitete ML Bibliothek für Python ist **Scikit-Learn** (oder auch *sklearn*).\n",
    "Eine Besonderheit von Scikit-Learn ist, dass die Bibliothek sehr umfassend ist und ein breites Spektrum von ML-Algorithmen unterstützt.\n",
    "Darunter sind Algorithmen zur Klassifikation, Regression und für das Clustering.\n",
    "Auch unterstützende Methode wie etwa Algorithmen zur Dimensionsreduktion, zur Modellauswahl und zur Vorverarbeitung der Rohdaten sind in der Bibliothek enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktionen zur linearen Regression entnehmen wir aus dem Modul `sklearn.linear_model`.\n",
    "Zur besseren Übersicht, weisen wir die abhängige Variable auf `y` (bzw. `y_test`) zu, die unabhängige Variable auf `X` (bzw. `X_test`)\n",
    "\n",
    "Wir erzeugen mit `lin_reg` ein Modell als Objekt der Klasse `LinearRegression` und trainieren das Modell mit dem Aufruf `fit(X,y)` auf unsere Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir das Modell trainiert, aber wie gut passt es zu unserem Datensatz?\n",
    "Eine Antwort aus diese Frage kann uns der durchschnittlichen Fehler für unseren Trainingsdatensatz geben.\n",
    "Die Funktion `mean_absolute_error` berechnet den Mittelwert der absoluten Fehler für die Schätzung `pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "avg_price = y_train.values.mean()\n",
    "print(\"Mittlerer Preis: %d EUR\" % avg_price)\n",
    "\n",
    "pred = lin_reg.predict(X_train)\n",
    "pred_err = mean_absolute_error(y_train,pred)\n",
    "pred_err_pc = pred_err*100//avg_price\n",
    "\n",
    "print(\"Mittlerer Fehler bei der Vorhersage: %d EUR (oder %d%%)\" % (pred_err, pred_err_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allerdings haben wir hier den Trainingsdatensatz `(X_train,y_train)` benutzt.\n",
    "Falls es beim Trainieren des Modells zu Overfitting-Problemen gekommen ist, würden wir diese so nicht erkennen.\n",
    "Daher führen wir die Fehlerberechnung nun nochmals mit dem reservierten Testdatensatz aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price = y_test.values.mean()\n",
    "print(\"Mittlerer Preis: %d EUR\" % avg_price)\n",
    "\n",
    "pred = lin_reg.predict(X_test)\n",
    "pred_err = mean_absolute_error(y_test,pred)\n",
    "pred_err_pc = pred_err*100//avg_price\n",
    "\n",
    "print(\"Mittlerer Fehler bei der Vorhersage: %d EUR (oder %d%%)\" % (pred_err, pred_err_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gute Nachricht ist, dass unser Modell auch für neue Eingaben gleichartige Ergebnisse produziert.\n",
    "Allerdings ist die Qualität der Schätzungen mit über 20%-igen Abweichung noch nicht optimal.\n",
    "Das liegt vor allem daran, dass sich der Preis eines Gebrauchtwagens nicht nur an einem Merkmal festmachen lässt.\n",
    "Daher werden wir im späteren Verlauf die Regression erneut, dann aber mit mehreren unabhängigen Variablen durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst bietet es sich aber an, die trainierte Funktion zu zeichnen.\n",
    "Dazu benötigen wir die berechneten Parameter $\\Theta$ der linearen Funktion $h_{\\Theta}(x)=\\Theta_0+\\Theta_1x$.\n",
    "\n",
    "Der *Niveauparameter* (oder *Achsenabschnitt*) $\\Theta_0$ wird im Modell im Attribut `intercept_` abgelegt.\n",
    "Alle weiteren Koeffizienten $\\Theta_i$ finden sich im Array `coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Theta_0 = \",  lin_reg.intercept_.item())\n",
    "print(\"Theta_1 = \", lin_reg.coef_.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen beiden Koeffizienten können wir die Modellfunktion $h$ aufstellen und über der Punktwolke des Datensatzes anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lin_reg.intercept_+lin_reg.coef_*X_test\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test,y_test)\n",
    "ax.plot(X_test, h,'-r')\n",
    "ax.set_xlabel(\"km Stand\")\n",
    "ax.set_ylabel(\"Preis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Verwenden Sie die gleiche Methode um die Abhängigkeit von `preis` und `leistung` zu untersuchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33c15c94442b71e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# X: Unabhängige Variable\n",
    "# y: Abhängige Variable\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "lin_reg = None\n",
    "\n",
    "# Diese Parameter für train_test_split wählen\n",
    "test_size=0.3\n",
    "random_state=42\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "(lin_reg.intercept_, lin_reg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57d6124e81369847",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann die Modellfunktion $h$ gebildet und geplottet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lin_reg.intercept_+lin_reg.coef_*X_test\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test,y_test)\n",
    "ax.plot(X_test, h,'-r')\n",
    "ax.set_xlabel(\"leistung\")\n",
    "ax.set_ylabel(\"Preis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Lineare Regression\n",
    "\n",
    "Nun wiederholen wir den Prozess und nehmen statt eine unabhängige Variable, die beiden Merkmale `kilometer`und `leistung` zum Modell hinzu.\n",
    "Damit wird unser Modell zu einer **Multiplen Linearen Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = golf_6_num['preis']\n",
    "X = golf_6_num[['kilometer','leistung']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "                  \n",
    "lin_reg2 = LinearRegression()\n",
    "lin_reg2.fit(X_train, y_train)\n",
    "\n",
    "avg_price = y_test.values.mean()\n",
    "print(\"Mittlerer Preis: %d EUR\" % avg_price)\n",
    "\n",
    "pred = lin_reg2.predict(X_test)\n",
    "pred_err = mean_absolute_error(y_test,pred)\n",
    "pred_err_pc = pred_err*100//avg_price\n",
    "\n",
    "print(\"Mittlerer Fehler bei der Vorhersage: %d EUR (oder %d%%)\" % (pred_err, pred_err_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht, dass sich die Schätzfunktion etwas verbessert hat.\n",
    "Der mittlere Fehler ist von 21% auf 17% schrumpft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir plotten nun auch die Modellfunktion für die Multiplen Lineare Regression mit zwei Parametern.\n",
    "Wie wir sehen, spannt unsere Modellfunktion nun eine Ebene auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from time import time\n",
    "%matplotlib notebook \n",
    "\n",
    "fig = plt.figure(figsize=(10,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "theta0 = lin_reg2.intercept_\n",
    "theta1 = lin_reg2.coef_[0]\n",
    "theta2 = lin_reg2.coef_[1]\n",
    "(theta0, theta1, theta2)\n",
    "\n",
    "\n",
    "# Erzeuge ein Gitter über die Fläche\n",
    "n = 100\n",
    "mn = np.min(X_test.values, axis=0)\n",
    "mx = np.max(X_test.values, axis=0)\n",
    "\n",
    "X,Y = np.linspace(mn[0], mx[0], n).reshape(1,-1), np.linspace(mn[1], mx[1], n).reshape(-1,1)\n",
    "Z = theta0 + theta1*X + theta2*Y\n",
    "\n",
    "ax.plot_surface(X, Y, Z, color='r', alpha=0.4)\n",
    "ax.scatter(X_test[\"kilometer\"], X_test[\"leistung\"], y_test)\n",
    "ax.view_init(20,800)\n",
    "ax.set_xlabel(\"Kilometer\")\n",
    "ax.set_ylabel(\"Leistung\")\n",
    "ax.set_zlabel(\"Preis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun nehmen wir mit dem Erstzulassungsjahr noch ein drittes Merkmal hinzu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y = golf_6_num['preis']\n",
    "X = golf_6_num.drop('preis', 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                  \n",
    "lin_reg3 = LinearRegression(normalize=True)\n",
    "lin_reg3.fit(X, y)\n",
    "\n",
    "avg_price = y_test.values.mean()\n",
    "print(\"Mittlerer Preis: %d EUR\" % avg_price)\n",
    "\n",
    "pred = lin_reg3.predict(X_test)\n",
    "pred_err = mean_absolute_error(y_test,pred)\n",
    "pred_err_pc = pred_err*100//avg_price\n",
    "\n",
    "print(\"Mittlerer Fehler bei der Vorhersage: %d EUR (oder %d%%)\" % (pred_err, pred_err_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erneut hat sich der mittlere Fehler verringert und mit 10% Fehlerquote gibt unsere Modellfunktion nun ganz passable Schätzungen.\n",
    "Versuchen Sie, die Funktion weiter zu optimieren, etwa indem Sie weitere Parameter hinzunehmen oder geschickt  neue Merkmale aus bestehenden Merkmalen berechnen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
